{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as pt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toma un conjunto de datos y lo separa en conjunto de datos positivos y conjunto de datos negtivos\n",
    "def split_positive_negative(dataframe):\n",
    "    positive_df = dataframe[dataframe['Dx:Cancer'] == 1]\n",
    "    negative_df = dataframe[dataframe['Dx:Cancer'] == 0]\n",
    "    return positive_df, negative_df\n",
    "\n",
    "# Toma el conjunto de datos, el numero de muestras que deseas y si queremos o no reemplazamiento.\n",
    "# Devuelve un conjunto de datos con las muestras seleccionadas.\n",
    "def random_sample(dataframe, num_samples, replacement=False):\n",
    "    list_values = [*range(dataframe.shape[0])]\n",
    "    if replacement==True:\n",
    "        values = random.choices(list_values, k=num_samples)\n",
    "    else:\n",
    "        values = random.sample(list_values, k=num_samples)\n",
    "    sampled_df = dataframe.iloc[values]\n",
    "    return sampled_df\n",
    "\n",
    "# Toma un conjunto de datos y lo separa en x (variable cancer) e y (resto de variables), y lo convierte a np.array\n",
    "def split_xy(dataframe):\n",
    "    df_x = dataframe.copy()\n",
    "    df_x.pop(\"Dx:Cancer\")\n",
    "    y = dataframe[\"Dx:Cancer\"].to_numpy()\n",
    "    x = df_x.to_numpy()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(algorithm, dataframe, num_samples, metric):\n",
    "    # Separamos los datos entre positivos y negativos\n",
    "    positive_df, negative_df = split_positive_negative(dataframe)\n",
    "    # Tomamos k muestras aleatorias de los positivos\n",
    "    positive_df = random_sample(positive_df, num_samples)\n",
    "    # Tomamos k muestras aleatorias de los negativos\n",
    "    negative_df = random_sample(negative_df, num_samples)\n",
    "    # Juntamos todas las muestras en un conjunto\n",
    "    df = pd.concat([positive_df, negative_df])\n",
    "    # Dividimos el conjunto en x e y\n",
    "    x_train, y_train = split_xy(df)\n",
    "    # Elegimos el algoritmo\n",
    "    if algorithm == 'NaiveBayes':\n",
    "        model = GaussianNB()\n",
    "    elif algorithm == 'DecisionTree':\n",
    "        model = DecisionTreeClassifier()\n",
    "    elif algorithm == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=100, max_features='auto')\n",
    "    elif algorithm == 'SVM':\n",
    "        model = svm.SVC(probability=True)\n",
    "    # Entrenamos el algoritmo\n",
    "    model.fit(x_train, y_train)\n",
    "    # Preparamos el conjunto de validacion\n",
    "    x_test, y_test = split_xy(dataframe)\n",
    "    # Evaluamos el algoritmo en el conjunto de test\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Calculamos la mtrica elegida\n",
    "    if metric == 'f1-score':\n",
    "        metric = metrics.f1_score(y_test, y_pred)\n",
    "    elif metric == 'classification_report':\n",
    "        metric = metrics.classification_report(y_test, y_pred, output_dict = True)\n",
    "    elif metric == 'confusion_matrix':\n",
    "        metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    elif metric == 'auc_score':\n",
    "        try:\n",
    "            metric = metrics.roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    elif metric == 'roc_curve':\n",
    "        metric = metrics.plot_roc_curve(model, x_test, y_test)\n",
    "        pt.show()\n",
    "        return None\n",
    "    return metric\n",
    "\n",
    "def evaluar_todo_el_set_de_datos(algorithm, dataframe, validation_split, metric):\n",
    "    # Separamos el conjunto entre x e y\n",
    "    x, y = split_xy(dataframe)\n",
    "    # Dividimos entre conjunto de entrenamiento y conjunto de validacion\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=validation_split)\n",
    "    sss.get_n_splits(x, y)\n",
    "    for train_index, test_index in sss.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    \n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=validation_split)\n",
    "    # Elegimos el algoritmo\n",
    "    if algorithm == 'NaiveBayes':\n",
    "        model = GaussianNB()\n",
    "    elif algorithm == 'DecisionTree':\n",
    "        model = DecisionTreeClassifier()\n",
    "    elif algorithm == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=100, max_features='auto')\n",
    "    elif algorithm == 'SVM':\n",
    "        model = svm.SVC(probability=True)\n",
    "    # Entrenamos el algoritmo\n",
    "    model.fit(x_train, y_train)\n",
    "    # Evaluamos el algoritmo en el conjunto de test\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Calculamos la metrica elegida\n",
    "    if metric == 'f1-score':\n",
    "        metric = metrics.f1_score(y_test, y_pred)\n",
    "    elif metric == 'classification_report':\n",
    "        metric = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "    elif metric == 'confusion_matrix':\n",
    "        metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    elif metric == 'auc_score':\n",
    "        #try:\n",
    "        metric = metrics.roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "        #except ValueError:\n",
    "        #    return 0\n",
    "    elif metric == 'roc_curve':\n",
    "        metric = metrics.plot_roc_curve(model, x_test, y_test)\n",
    "        pt.show()\n",
    "        return None\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Media de 500 evaluaciones 18 vs 18 muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES: \n",
      "Media de f1 score: 0.14171510225157685\n",
      "Media de auc score: 0.9190541155070487\n",
      "Media de confusion matrix: [[439.4 293.6]\n",
      " [  0.   18. ]]\n",
      "Media de classification report: {'0': {'precision': 0.9987581860937416, 'recall': 0.7092769440654842, 'f1-score': 0.7886564475496061}, '1': {'precision': 0.1126913634528031, 'recall': 0.9555555555555557, 'f1-score': 0.1978549767015424}}\n",
      " --------------------------------------------\n",
      "DECISION TREE: \n",
      "Media de f1 score: 0.6709913490764554\n",
      "Media de auc score: 0.9851978171896316\n",
      "Media de confusion matrix: [[711.4  21.6]\n",
      " [  0.   18. ]]\n",
      "Media de classification report: {'0': {'precision': 1.0, 'recall': 0.9748976807639836, 'f1-score': 0.9872508201716409}, '1': {'precision': 0.5182481940144479, 'recall': 1.0, 'f1-score': 0.6765274725274726}}\n",
      " --------------------------------------------\n",
      "RANDOM FOREST: \n",
      "Media de f1 score: 0.523258648098043\n",
      "Media de auc score: 0.9994959830225859\n",
      "Media de confusion matrix: [[697.1  35.9]\n",
      " [  0.   18. ]]\n",
      "Media de classification report: {'0': {'precision': 1.0, 'recall': 0.9529331514324693, 'f1-score': 0.9758306890466504}, '1': {'precision': 0.36028117115010005, 'recall': 1.0, 'f1-score': 0.5246448266173307}}\n",
      " --------------------------------------------\n",
      "SUPPORT VECTOR MACHINE: \n",
      "Media de f1 score: 0.09179198398588587\n",
      "Media de auc score: 0.6972184326208882\n",
      "Media de confusion matrix: [[488.1 244.9]\n",
      " [  5.8  12.2]]\n",
      "Media de classification report: {'0': {'precision': 0.9889586019946137, 'recall': 0.668212824010914, 'f1-score': 0.7960427451417014}, '1': {'precision': 0.050160056863705595, 'recall': 0.6944444444444445, 'f1-score': 0.09324492992801939}}\n",
      " --------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"datos.xlsx\")\n",
    "num_evaluaciones = 500\n",
    "\n",
    "# NAIVE BAYES\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += analysis('NaiveBayes', df, 18, 'f1-score')\n",
    "    auc_score += analysis('NaiveBayes', df, 18, 'auc_score')\n",
    "    confusion_matrix += analysis('NaiveBayes', df, 18, 'confusion_matrix')\n",
    "    report = analysis('NaiveBayes', df, 18, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"NAIVE BAYES: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "#DECISION TREE\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += analysis('DecisionTree', df, 18, 'f1-score')\n",
    "    auc_score += analysis('DecisionTree', df, 18, 'auc_score')\n",
    "    confusion_matrix += analysis('DecisionTree', df, 18, 'confusion_matrix')\n",
    "    report = analysis('DecisionTree', df, 18, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"DECISION TREE: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "# RANDOM FOREST\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += analysis('RandomForest', df, 18, 'f1-score')\n",
    "    auc_score += analysis('RandomForest', df, 18, 'auc_score')\n",
    "    confusion_matrix += analysis('RandomForest', df, 18, 'confusion_matrix')\n",
    "    report = analysis('RandomForest', df, 18, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"RANDOM FOREST: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "# SUPPORT VECTOR MACHINE\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += analysis('SVM', df, 18, 'f1-score')\n",
    "    auc_score += analysis('SVM', df, 18, 'auc_score')\n",
    "    confusion_matrix += analysis('SVM', df, 18, 'confusion_matrix')\n",
    "    report = analysis('SVM', df, 18, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"SUPPORT VECTOR MACHINE: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media de 500 evaluaciones clases balanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES: \n",
      "Media de f1 score: 0.6952832268247378\n",
      "Media de auc score: 0.7297727272727272\n",
      "Media de confusion matrix: [[ 24.6 195.4]\n",
      " [  0.  222. ]]\n",
      "Media de classification report: {'0': {'precision': 1.0, 'recall': 0.12272727272727271, 'f1-score': 0.21811627099567113}, '1': {'precision': 0.5349948906255181, 'recall': 1.0, 'f1-score': 0.6970476667116243}}\n",
      " --------------------------------------------\n",
      "DECISION TREE: \n",
      "Media de f1 score: 0.9973134141209012\n",
      "Media de auc score: 0.9965909090909092\n",
      "Media de confusion matrix: [[218.3   1.7]\n",
      " [  0.  222. ]]\n",
      "Media de classification report: {'0': {'precision': 1.0, 'recall': 0.9922727272727272, 'f1-score': 0.9961160853455608}, '1': {'precision': 0.9924204579168864, 'recall': 1.0, 'f1-score': 0.996190826324257}}\n",
      " --------------------------------------------\n",
      "RANDOM FOREST: \n",
      "Media de f1 score: 0.9984279740011084\n",
      "Media de auc score: 1.0\n",
      "Media de confusion matrix: [[219.   1.]\n",
      " [  0. 222.]]\n",
      "Media de classification report: {'0': {'precision': 1.0, 'recall': 0.9972727272727273, 'f1-score': 0.9986322172642265}, '1': {'precision': 0.9973134208840488, 'recall': 1.0, 'f1-score': 0.9986526931022321}}\n",
      " --------------------------------------------\n",
      "SUPPORT VECTOR MACHINE: \n",
      "Media de f1 score: 0.8193602551073731\n",
      "Media de auc score: 0.9334479934479933\n",
      "Media de confusion matrix: [[189.1  30.9]\n",
      " [ 50.3 171.7]]\n",
      "Media de classification report: {'0': {'precision': 0.7982062093712534, 'recall': 0.8672727272727274, 'f1-score': 0.8309220930124926}, '1': {'precision': 0.8564362398865646, 'recall': 0.7815315315315317, 'f1-score': 0.8167215110618938}}\n",
      " --------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"datos.xlsx\")\n",
    "positive_df, negative_df = split_positive_negative(df)\n",
    "for i in range(int(negative_df.shape[0]/positive_df.shape[0])):\n",
    "    df = pd.concat([df, positive_df])\n",
    "\n",
    "num_evaluaciones = 500\n",
    "\n",
    "# NAIVE BAYES\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"NAIVE BAYES: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "#DECISION TREE\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"DECISION TREE: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "# RANDOM FOREST\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"RANDOM FOREST: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "# SUPPORT VECTOR MACHINE\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"SUPPORT VECTOR MACHINE: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media de 500 evaluaciones con todo el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES: \n",
      "Media de f1 score: 0.045521091508614484\n",
      "Media de auc score: 0.6847963800904978\n",
      "Media de confusion matrix: [[ 54.3 166.7]\n",
      " [  0.7   4.3]]\n",
      "Media de classification report: {'0': {'precision': 0.9966666666666667, 'recall': 0.12398190045248869, 'f1-score': 0.21734310202491586}, '1': {'precision': 0.024089797228478392, 'recall': 0.96, 'f1-score': 0.046996958317969226}}\n",
      " --------------------------------------------\n",
      "DECISION TREE: \n",
      "Media de f1 score: 0.6441308691308691\n",
      "Media de auc score: 0.8877375565610862\n",
      "Media de confusion matrix: [[220.3   0.7]\n",
      " [  1.4   3.6]]\n",
      "Media de classification report: {'0': {'precision': 0.991905720063756, 'recall': 0.9932126696832579, 'f1-score': 0.9925373681082466}, '1': {'precision': 0.7344047619047619, 'recall': 0.6399999999999999, 'f1-score': 0.6486113886113886}}\n",
      " --------------------------------------------\n",
      "RANDOM FOREST: \n",
      "Media de f1 score: 0.6684415584415584\n",
      "Media de auc score: 0.9923076923076923\n",
      "Media de confusion matrix: [[220.1   0.9]\n",
      " [  1.5   3.5]]\n",
      "Media de classification report: {'0': {'precision': 0.9932450991642169, 'recall': 0.9963800904977376, 'f1-score': 0.9948050345470106}, '1': {'precision': 0.8316666666666667, 'recall': 0.7, 'f1-score': 0.7476479076479077}}\n",
      " --------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORT VECTOR MACHINE: \n",
      "Media de f1 score: 0.0\n",
      "Media de auc score: 0.9772850678733033\n",
      "Media de confusion matrix: [[221.   0.]\n",
      " [  5.   0.]]\n",
      "Media de classification report: {'0': {'precision': 0.9778761061946903, 'recall': 1.0, 'f1-score': 0.9888143176733781}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0}}\n",
      " --------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\navar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"datos.xlsx\")\n",
    "num_evaluaciones = 500\n",
    "\n",
    "# NAIVE BAYES\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('NaiveBayes', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"NAIVE BAYES: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "#DECISION TREE\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('DecisionTree', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"DECISION TREE: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "# RANDOM FOREST\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('RandomForest', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"RANDOM FOREST: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")\n",
    "\n",
    "# SUPPORT VECTOR MACHINE\n",
    "f1_score = 0.0\n",
    "auc_score = 0.0\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "classification_report = {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score':0.0}}\n",
    "for i in range(num_evaluaciones):\n",
    "    f1_score += evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'f1-score')\n",
    "    auc_score += evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'auc_score')\n",
    "    confusion_matrix += evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'confusion_matrix')\n",
    "    report = evaluar_todo_el_set_de_datos('SVM', df, 0.3, 'classification_report')\n",
    "    classification_report['0']['precision'] += report['0']['precision']\n",
    "    classification_report['0']['recall'] += report['0']['recall']\n",
    "    classification_report['0']['f1-score'] += report['0']['f1-score']\n",
    "    classification_report['1']['precision'] += report['1']['precision']\n",
    "    classification_report['1']['recall'] += report['1']['recall']\n",
    "    classification_report['1']['f1-score'] += report['1']['f1-score']\n",
    "f1_score /= num_evaluaciones\n",
    "auc_score /= num_evaluaciones\n",
    "confusion_matrix /= num_evaluaciones\n",
    "classification_report['0']['precision'] /= num_evaluaciones\n",
    "classification_report['0']['recall'] /= num_evaluaciones\n",
    "classification_report['0']['f1-score'] /= num_evaluaciones\n",
    "classification_report['1']['precision'] /= num_evaluaciones\n",
    "classification_report['1']['recall'] /= num_evaluaciones\n",
    "classification_report['1']['f1-score'] /= num_evaluaciones\n",
    "print(\"SUPPORT VECTOR MACHINE: \")\n",
    "print(\"Media de f1 score: \" + str(f1_score))\n",
    "print(\"Media de auc score: \" + str(auc_score))\n",
    "print(\"Media de confusion matrix: \" + str(confusion_matrix))\n",
    "print(\"Media de classification report: \" + str(classification_report))\n",
    "print(\" --------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
